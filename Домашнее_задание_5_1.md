## Задание: Парсинг новостных сайтов и построение базы статей

Цель:
Собрать корпус новостных статей с русскоязычных сайтов, сохранить их в базу данных SQLite и продемонстрировать умение работать с веб-парсингом, ограничениями сайтов и очисткой текста.

------

### 1. Общие требования

1. Необходимо спарсить данные с русскоязычных новостных сайтов.
   Примеры (не ограничивайтесь ими):
   - ria.ru
   - lenta.ru
   - naked-science.ru
   - ngs24.ru
   - региональные новостные порталы
   - отраслевые/специализированные сайты (экономика, IT, медицина, строительство и т.д.)

2. Язык и платформа парсинга — любые.

3. Минимальное количество записей в базе:
   - не менее 5000 статей.

4. Требование к источникам внутри одной группы:
   - источники должны быть разнообразными;
   - допускается максимум 2 совпадающих (не уникальных) источника между разными командами/парами;
   - по остальным сайтам каждая группа должна постараться использовать свои уникальные источники.

5. Разрешается выполнять задание в парах

------
### 2. Требования к базе данных

Необходимо создать базу данных SQLite с таблицей (название на ваше усмотрение), содержащей поля:

- `guid` – уникальный идентификатор записи, тип UUID v4
  - генерируется программно (например, в Python: `uuid.uuid4()`)

- `title` – заголовок статьи

- `description` – очищенный текст статьи
  - без HTML-тегов
  - без встроенного медиа-контента (аудио, видео, изображения)
  - переносы строк допускается хранить как `\n`

- `url` – ссылка на статью

- `published_at` – дата/время публикации статьи на сайте
  - формат можно выбрать самостоятельно, но нужно использовать единообразно (например, `TEXT` в ISO формате или `INTEGER` как Unix timestamp)

- `comments_count` – количество комментариев к статье
  - если данные недоступны, можно хранить `NULL` или `0`, но нужно быть последовательными

- `created_at_utc` – отметка времени (UTC), когда запись была сохранена в БД
  - timestamp на момент вставки в базу

- `rating` – оценка/рейтинг/лайки, если есть на сайте

  - если нет — допускается `NULL` или `0`

> Обратите внимание: если на сайте нет некоторых полей (rating, comments_count и т.п.), продумайте стратегию заполнения (значение по умолчанию, NULL и т.д.).

------
### 3. Требования к обработке и очистке текста
1. Текст статьи необходимо очистить от HTML-контента:
   - убрать все HTML-теги (`<p>`, `<br>`, `<a>`, `<div>` и т.д.);
   - удалить/игнорировать вставки встроенного контента:
     * аудио-плееры
     * видеоплееры
     * блоки с изображениями/галереями
   - на выходе должен быть чистый читабельный текст.

2. Если статья полностью состоит из встроенного контента (например, только видео и подпись из одного слова) — такую статью пропускаем и в базу не записываем.

3. Переносы строк можно кодировать как символ `\n`.

------
### 4. Ограничения и работа с rate limit

При разработке парсера необходимо учитывать реальные ограничения сайтов:
1. Rate limiting:
   - нельзя отправлять слишком много запросов одновременно;
   - нужно явно ограничить частоту запросов (например, не более N запросов в секунду);
   - для языков с поддержкой асинхронности обязательно используйте:
     - семафоры/пулы соединений (например, `asyncio.Semaphore` в Python)
     - или искусственные задержки (sleep) между запросами.

2. User-Agent:
   - не используйте дефолтный User-Agent библиотеки;
   - задайте собственный User-Agent;
   - желательно иметь несколько вариантов и случайно выбирать один (чтобы снизить вероятность блокировки по User-Agent).

3. При ошибках/блокировках от сайта:
   - обрабатывать HTTP-коды ошибок;
   - при необходимости реализовать повтор запроса (retry) с задержкой.

------
### 5. Результаты, которые нужно предоставить
1. Исходный код проекта:
   - выложить в репозиторий;
   - структура проекта должна быть понятной:
     - код парсинга
     - код создания/заполнения БД
     - при необходимости — отдельные вспомогательные скрипты

2. Файл базы данных articles:
   - файл `.sqlite` / `.db` / другое расширение SQLite;
   - не менее 5000 записей в таблице статей;
   - поля должны соответствовать описанным требованиям.

3. Краткое объяснение о последовательноысти действий:
   - как выбирались сайты;
   - как организован парсинг (структура кода, библиотеки);
   - как учитывались ограничения (rate limit, user-agent);
   - как выполнялась очистка текста от HTML и медиа;
   - как формировалась/заполнялась БД.

--------

### 6. Схема оценивания (максимум 5 баллов)

1. Исходный код
   - наличие кода в репозитории;
   - понятная структура, комментарии, читаемость;
   - уникальность решения (максимум 2 похожих решения в группе/паре);
   - отсутствие заимствования кода/БД у других групп. (обнуление)

2. База данных и качество данных
   - наличие файла SQLite;
   - количество записей ≥ 5000;
   - данные соответствуют схеме (все поля, корректные типы);
   - текст статей очищен от HTML и лишнего контента;

3. Объяснение последовательности действий:
   - чёткое описание, что и как было сделано;
   - понимание студентом своего кода и процесса;
   - возможность устно объяснить, как данные были получены и обработаны.
